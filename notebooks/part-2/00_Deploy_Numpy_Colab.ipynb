{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nzMOhVNy-Qv"
      },
      "source": [
        "# Model Deployment using Numpy Linear Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSyHRB_qy-Qw"
      },
      "source": [
        "## 1. Introduction\n",
        "In this workbook, we will look into the basics of deploying a model. For simplicity, we will consider a simple numpy linear classifier $$ \\mathbf{Y} = \\mathbf{W} \\mathbf{X} + \\mathbf{b}$$\n",
        "\n",
        "For simplicity, we will consider $\\mathbf{X}$ to be 6 dimensional ($\\mathbb{R}^6$). i.e. 1 data point $x \\in \\mathbf{X}$ will be a numpy array of shape $(1,6)$. The output $\\mathbf{Y}$ is 3 dimensional ($\\mathbb{R}^3$). Then, the weights $\\mathbf{W}$ will be a numpy array of shape $(3,6)$ and bias $\\mathbf{b}$ will be a numpy array of shape $(,3)$. \n",
        "\n",
        "In this workbook, we will demonstrate how to deploy this numpy linear classifier as a server and how to perform query on this numpy linear classifier.\n",
        "\n",
        "## 2. Imports and Dependencies.\n",
        "The few packages needed are loaded next. Particularly, `numpy`, `mlflow` will be majorly used in this tutorial. `requests` package will be used for performing query. `json` is used to post and get response from the server."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://stackoverflow.com/questions/61615818/setting-up-mlflow-on-google-colab\n",
        "!pip install mlflow --quiet\n",
        "!pip install pyngrok --quiet"
      ],
      "metadata": {
        "id": "WZEjptoC6EAs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91488cba-c01d-4a64-d67b-af79b7414d5f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 16.5 MB 110 kB/s \n",
            "\u001b[K     |████████████████████████████████| 210 kB 31.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 36.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 62 kB 566 kB/s \n",
            "\u001b[K     |████████████████████████████████| 146 kB 38.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 40.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 78 kB 5.9 MB/s \n",
            "\u001b[?25h  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 745 kB 24.1 MB/s \n",
            "\u001b[?25h  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "\n",
        "with mlflow.start_run(run_name=\"MLflow on Colab\"):\n",
        "  mlflow.log_metric(\"m1\", 2.0)\n",
        "  mlflow.log_param(\"p1\", \"mlflow-colab\")\n",
        "\n",
        "# run tracking UI in the background\n",
        "get_ipython().system_raw(\"mlflow ui --port 5000 &\") # run tracking UI in the background\n",
        "\n",
        "\n",
        "# create remote tunnel using ngrok.com to allow local port access\n",
        "# borrowed from https://colab.research.google.com/github/alfozan/MLflow-GBRT-demo/blob/master/MLflow-GBRT-demo.ipynb#scrollTo=4h3bKHMYUIG6\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "\n",
        "# Setting the authtoken (optional)\n",
        "# Get your authtoken from https://dashboard.ngrok.com/auth\n",
        "NGROK_AUTH_TOKEN = \"27XxLnFfAI8offnpHao1vsKyWja_4uTdALriviWk545yXPDFW\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Open an HTTPs tunnel on port 5000 for http://localhost:5000\n",
        "ngrok_tunnel = ngrok.connect(addr=\"5000\", proto=\"http\", bind_tls=True)\n",
        "print(\"MLflow Tracking UI:\", ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "id": "psqYniMO3FqE",
        "outputId": "2ca7911f-44aa-475e-bc0a-deac84352141",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLflow Tracking UI: https://b7ba-34-85-170-199.ngrok.io\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CM8nJ8tFy-Qx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import mlflow\n",
        "import numpy as np\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "dinI7RQVy-Qx"
      },
      "source": [
        "## MLflow for experiment tracking and model deployment\n",
        "\n",
        "MLflow is an open source platform for managing the end-to-end machine learning lifecycle. It tackles four primary functions:\n",
        "\n",
        "- Tracking experiments to record and compare parameters and results (MLflow Tracking).\n",
        "- Managing and deploying models from a variety of ML libraries to a variety of model serving and inference platforms (MLflow Models).\n",
        "- Providing a central model store to collaboratively manage the full lifecycle of an MLflow Model, including model versioning, stage transitions, and annotations (MLflow Model Registry).\n",
        "\n",
        "More information [here](https://www.mlflow.org/docs/latest/index.html#)\n",
        "\n",
        "\n",
        "\n",
        "![image.png](https://www.mlflow.org/docs/latest/_images/scenario_4.png)\n",
        "\n",
        "- localhost maps to the server on which the current notebook is running\n",
        "\n",
        "- Tracking server maps to the server at environment variable `TRACKING_URL` that can be printed using `os.environ.get(\"TRACKING_URL\")`\n",
        "\n",
        "- Create an mlflow client that communicates with the tracking server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HSh0W7P5y-Qy"
      },
      "outputs": [],
      "source": [
        "from mlflow import pyfunc\n",
        "\n",
        "# Setting a tracking uri to log the mlflow logs in a particular location tracked by \n",
        "from mlflow.tracking import MlflowClient\n",
        "tracking_uri = os.environ.get(\"TRACKING_URL\")\n",
        "client = MlflowClient(tracking_uri=tracking_uri)\n",
        "mlflow.set_tracking_uri(tracking_uri)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "f6l5SknmzObw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg63m3i6y-Qy"
      },
      "source": [
        "## Create an experiment in mlflow database using mlflow client\n",
        "\n",
        "- Get the list of all the experiments (Click on **Experiments** tab on the sidebar to see the list)\n",
        "- Create a new experiment named *numpy_deployment* if it doesn't exist\n",
        "- Set *numpy_deployment* as the new experiment under which different **runs** are tracked"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "fC7brTn6y-Qy"
      },
      "source": [
        "## MLflow Entity Hierarchy\n",
        "\n",
        "- Experiment 1\n",
        "    - Run 1\n",
        "        - Parameters\n",
        "        - Metrics\n",
        "        - Artifacts\n",
        "            - Folder 1\n",
        "                - File 1\n",
        "                - File 2\n",
        "            - Folder 2 \n",
        "    - Run 2\n",
        "    - Run 3\n",
        "\n",
        "- Experiment 2\n",
        "- Experiment 3        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "U4T8H9Nry-Qz",
        "outputId": "c48f2b12-d803-414c-f956-afa86b059520",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='file:///content/mlruns/1', experiment_id='1', lifecycle_stage='active', name='numpy_deployment', tags={}>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Setting a tracking project experiment name to keep the experiments organized\n",
        "experiments = client.list_experiments()\n",
        "experiment_names = []\n",
        "for exp in experiments:\n",
        "    experiment_names.append(exp.name)\n",
        "experiment_name = \"numpy_deployment\"\n",
        "if experiment_name not in experiment_names:\n",
        "    mlflow.create_experiment(experiment_name)\n",
        "mlflow.set_experiment(experiment_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "73SWebkDy-Qz"
      },
      "source": [
        "## Python Class for inference\n",
        "\n",
        "- ModelWrapper is derived from mlflow.pyfunc.PythonModel [more info](https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html)\n",
        "- load_context() member function is used to load the model. In this case, it loads a numpy file with two arrays **weights** and **bias**\n",
        "- predict member function takes a numpy array as input and outputs another numpy array\n",
        "- An object of this class will be saved as a pickle file in blob storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "p7YhqpkUy-Qz"
      },
      "outputs": [],
      "source": [
        "## Model Wrapper that takes \n",
        "class ModelWrapper(mlflow.pyfunc.PythonModel):\n",
        "    def load_context(self,context):\n",
        "        import numpy as np\n",
        "        self.model = np.load(context.artifacts['model_path'], allow_pickle=True).tolist()\n",
        "        print(\"Model initialized\")\n",
        "    \n",
        "    def predict(self, context, model_input):\n",
        "        import numpy as np\n",
        "        import json\n",
        "        json_txt = \", \".join(model_input.columns)\n",
        "        data_list = json.loads(json_txt)\n",
        "        inputs = np.array(data_list)\n",
        "        if len(inputs.shape) == 2:\n",
        "            print('batch inference')\n",
        "            predictions = []\n",
        "            for idx in range(inputs.shape[0]):\n",
        "                prediction = np.matmul(inputs[idx,:],self.model['weights'].T) + self.model['bias']\n",
        "                predictions.append(prediction.tolist())\n",
        "        elif len(inputs.shape) == 1:\n",
        "            print('single inference')\n",
        "            predictions = self.model['weights'].T * inputs + self.model['bias']\n",
        "            predictions = predictions.tolist()\n",
        "        else:\n",
        "            raise ValueError('invalid input shape')\n",
        "        return json.dumps(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wbwpp3L7y-Q0"
      },
      "source": [
        "## Register a model using mlflow\n",
        "\n",
        "- Log user-defined parameters in a remote database through a remote server\n",
        "- Create a model_wrapper object using ModelWrapper() class in the above cell\n",
        "- Create a default conda environment that need to be installed on the Docker conatiner that serves a REST API\n",
        "- Save the model object as a pickle file and conda environment as artifacts (files) in S3 or Blob Storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "w3Vlvsjby-Q0"
      },
      "outputs": [],
      "source": [
        "# instantiate the python inference model wrapper for the server\n",
        "model_wrapper = ModelWrapper()\n",
        "\n",
        "\n",
        "# define the model weights randomly\n",
        "np_weights = np.random.rand(3,6)\n",
        "np_bias = np.random.rand(3)\n",
        "\n",
        "# checkpointing and logging the model in mlflow\n",
        "artifact_path = './np_model'\n",
        "np.save(artifact_path, {'weights':np_weights, 'bias':np_bias})\n",
        "model_artifacts = {\"model_path\" : artifact_path+'.npy'}\n",
        "\n",
        "#Conda environment\n",
        "env = mlflow.sklearn.get_default_conda_env()\n",
        "with mlflow.start_run():\n",
        "    mlflow.log_param(\"features\",6)\n",
        "    mlflow.log_param(\"labels\",3)\n",
        "    mlflow.pyfunc.log_model(\"np_model\", python_model=model_wrapper, artifacts=model_artifacts, conda_env=env)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Xd3WYLCb1Dyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjANmkN8y-Q0"
      },
      "source": [
        "## 5. Use the Endpoint and Query from the server\n",
        "\n",
        "There are two methods to perform query... The first is using `requests` library and the other using `curl` shell command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "H5mB3nr2y-Q1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb692b37-271c-4764-9d86-11bbf08091ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.2642828 1.417612  1.5717524]]\n",
            "[[1.7700912 1.3319434 2.044264 ]\n",
            " [2.5932946 1.8004078 3.0677977]\n",
            " [2.0653286 1.4503199 2.342546 ]\n",
            " [2.2049885 1.5345415 2.477913 ]\n",
            " [1.7142023 1.5396183 2.2291229]\n",
            " [2.2388797 1.4262313 2.5884798]\n",
            " [1.8614157 1.5010685 2.4475756]\n",
            " [1.7703766 1.569534  1.9948692]\n",
            " [1.5760703 1.6345834 2.1014514]\n",
            " [1.8516906 1.8806502 2.3388996]\n",
            " [1.3438103 1.2168763 1.6201899]\n",
            " [2.468498  1.741846  2.9562566]\n",
            " [2.3695126 1.6911329 2.5876129]\n",
            " [1.5991931 1.5471226 2.0437183]\n",
            " [2.4479113 1.6151091 2.8826597]\n",
            " [2.2748516 1.5989865 2.7707183]\n",
            " [1.502824  1.2764392 1.7962253]\n",
            " [1.848351  1.5082681 2.2302144]\n",
            " [2.0217803 1.6401912 2.5365727]\n",
            " [2.2394376 1.6383314 2.465779 ]]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "################################################################################\n",
        "# *** SET MODEL URL HERE BEFORE RUNNING THIS CELL (instructions above) ***\n",
        "# Example: http://127.0.0.1:5000/invocations\n",
        "url = \"http://localhost:5002/invocations\"\n",
        "################################################################################\n",
        "\n",
        "if not url:\n",
        "    raise ValueError('Model URL not set! Please read instructions on how to deploy model, set the correct URL, and try again.')\n",
        "\n",
        "headers = {\"Content-Type\":\"text/csv\"}\n",
        "\n",
        "# First case, run inference on single data point\n",
        "np_array = np.random.rand(1,6).tolist()\n",
        "json_data = json.dumps(np_array)\n",
        "\n",
        "if url:\n",
        "    response = requests.post(url,data=json_data,headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        output = np.array(json.loads(response.json())).astype(np.float32)\n",
        "        print(output)\n",
        "    else:\n",
        "        print(response.status_code)\n",
        "        print(\"REST API deployment is in progress -- please try again in a few minutes!\")\n",
        "else:\n",
        "    print(\"Make sure that the model is in ON state. Copy the Endpoint\")\n",
        "\n",
        "# Second case, run inference on multiple data points\n",
        "np_array = np.random.rand(20,6).tolist()\n",
        "json_data = json.dumps(np_array)\n",
        "\n",
        "if url:\n",
        "    response = requests.post(url,data=json_data,headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        output = np.array(json.loads(response.json())).astype(np.float32)\n",
        "        print(output)\n",
        "    else:\n",
        "        print(response.status_code)\n",
        "        print(\"REST API deployment is in progress -- please try again in a few minutes!\")\n",
        "else:\n",
        "    print(\"Make sure that the model is in ON state. Copy the Endpoint\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BSN8CSPvy-Q1",
        "outputId": "e86fecb7-1311-4b26-a9d5-c63db6575a0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022/04/09 05:04:29 INFO mlflow.models.cli: Selected backend for flavor 'python_function'\n",
            "2022/04/09 05:04:29 INFO mlflow.pyfunc.backend: === Running command 'gunicorn --timeout=60 -b 127.0.0.1:5002 -w 1 ${GUNICORN_CMD_ARGS} -- mlflow.pyfunc.scoring_server.wsgi:app'\n",
            "[2022-04-09 05:04:30 +0000] [351] [INFO] Starting gunicorn 20.1.0\n",
            "[2022-04-09 05:04:30 +0000] [351] [INFO] Listening at: http://127.0.0.1:5002 (351)\n",
            "[2022-04-09 05:04:30 +0000] [351] [INFO] Using worker: sync\n",
            "[2022-04-09 05:04:30 +0000] [354] [INFO] Booting worker with pid: 354\n",
            "Model initialized\n",
            "\n",
            "[2022-04-09 05:04:40 +0000] [351] [INFO] Handling signal: int\n",
            "Aborted!\n",
            "[2022-04-09 05:04:41 +0000] [354] [INFO] Worker exiting (pid: 354)\n",
            "[2022-04-09 05:04:41 +0000] [351] [INFO] Shutting down: Master\n"
          ]
        }
      ],
      "source": [
        "!mlflow models serve -m file:///content/mlruns/1/8a16c03605de48c08050cc64726d190f/artifacts/np_model --port 5002 --no-conda"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw(\"mlflow models serve -m file:///content/mlruns/1/8a16c03605de48c08050cc64726d190f/artifacts/np_model --port 5002 --no-conda &\") # run tracking UI in the background\n"
      ],
      "metadata": {
        "id": "lB13Bwzi8FBH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rjR0NJqM8O-2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "colab": {
      "name": "00 Deploy Numpy Colab.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}